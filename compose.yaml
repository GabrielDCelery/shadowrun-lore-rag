services:
  shadowrun-rag:
    build: .
    container_name: shadowrun-rag
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - DATA_PATH=/data
      - EMBEDDING_MODEL=mxbai-embed-large
      - LLM_MODEL=llama3.1:8b
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - TOP_K=12
      - LOG_LEVEL=DEBUG
    volumes:
      - /srv/shadowrun-rag/pdfs:/data/pdfs:ro
      - /srv/shadowrun-rag/extracted:/data/extracted
      - /srv/shadowrun-rag/chroma_db:/data/chroma_db
      - /srv/shadowrun-rag/model_cache:/root/.cache
    networks:
      - ollama_net
    # No ports needed - CLI only
    # Override command to keep running for manual exec
    command: tail -f /dev/null
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  ollama_net:
    external: true
