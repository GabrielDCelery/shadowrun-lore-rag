services:
  ollama-rag:
    image: ollama/ollama:0.15.1
    container_name: ollama-rag
    volumes:
      - /srv/ollama:/root/.ollama
    # https://docs.docker.com/compose/how-tos/gpu-support
    networks:
      - shadowrun_rag
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  shadowrun-rag:
    build: .
    container_name: shadowrun-rag
    environment:
      - OLLAMA_HOST=http://ollama-rag:11434
      - DATA_PATH=/data
      - EMBEDDING_MODEL=mxbai-embed-large
      - LLM_MODEL=llama3.1:8b
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - TOP_K=12
      - LOG_LEVEL=INFO
    volumes:
      - /srv/shadowrun-rag/pdfs:/data/pdfs:ro
      - /srv/shadowrun-rag/extracted:/data/extracted
      - /srv/shadowrun-rag/chroma_db:/data/chroma_db
      - /srv/shadowrun-rag/model_cache:/root/.cache
    networks:
      - shadowrun_rag
    # Override command to keep running for manual exec
    command: tail -f /dev/null
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

networks:
  shadowrun_rag:
    external: false
